{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Diagnostic Models for Precipitation\n",
    "\n",
    "The following notebook will demonstrate how to use diagnostic models inside of Earth-2\n",
    "MIP for transforming outputs of global weather models into different quantities of\n",
    "interest. More information on diagnostics can be found in the [user guide](https://nvidia.github.io/earth2mip/userguide/diagnostic.html).\n",
    "\n",
    "In summary this notebook will cover the following topics:\n",
    "\n",
    "- Loading a built in diagnostic model for predicting total precipitation\n",
    "- Combining the diagnostic model with a prognostic model using the DiangosticLooper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Diagnostic Models\n",
    "Loading diagnostic models is similar to prognostic models, but presently use a\n",
    "slightly different API. In this example we will using the built in FourCastNet AFNO\n",
    "to serve as the underlying prognostic model that will drive the time-integration. The\n",
    "`PrecipitationAFNO` model will then be used to \"post-process\" the outputs of\n",
    "this model to predict precipitation. The key API to load a diagnostic model is the\n",
    "`load_diagnostic(package)` function which takes a model package in. If you're\n",
    "interested in using the built in model package (i.e. checkpoint), then the\n",
    "`load_package()` function can do this for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from modulus.distributed.manager import DistributedManager\n",
    "from earth2mip.networks import get_model\n",
    "from earth2mip.diagnostic import PrecipitationAFNO\n",
    "\n",
    "device = DistributedManager().device\n",
    "\n",
    "print(\"Loading FCN model\")\n",
    "model = get_model(\"e2mip://fcn\", device=device)\n",
    "\n",
    "print(\"Loading precipitation model\")\n",
    "package = PrecipitationAFNO.load_package()\n",
    "diagnostic = PrecipitationAFNO.load_diagnostic(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to wrap the prognostic model with the Diagnostic Time loop.\n",
    "Essentially, this adds the execution of the diagnostic model on top of the forecast\n",
    "model iterator. This will add the total precipitation field (`tp`) to the output data\n",
    "which can the be further processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from earth2mip.diagnostic import DiagnosticTimeLoop\n",
    "\n",
    "model_diagnostic = DiagnosticTimeLoop(diagnostics=[diagnostic], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "With the diagnostic time loop created the final steps are to create the data source\n",
    "and run inference. For this example we will use the CDS data source again, using cached\n",
    "data when working with default dates. You can also try different dates, however it is assumed\n",
    "that your CDS API key is already set up. Reference the\n",
    "[first example](https://nvidia.github.io/earth2mip/examples/01_ensemble_inference.html#set-up)\n",
    "for additional information. We will use the basic inference workflow which returns a\n",
    "Xarray dataset we will save to netCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from earth2mip.inference_ensemble import run_basic_inference\n",
    "from earth2mip.initial_conditions import cds, gfs, hdf5\n",
    "\n",
    "print(\"Constructing initializer data source\")\n",
    "dat_source = 'cds' # 'cds', 'gfs'\n",
    "\n",
    "if dat_source == 'gfs':\n",
    "    data_source = gfs.DataSource(model.in_channel_names)\n",
    "    time = datetime.datetime(2024, 4, 9)\n",
    "elif dat_source == 'cds':\n",
    "    data_source = cds.DataSource(model.in_channel_names)\n",
    "    time = datetime.datetime(2017, 8, 24, 12)\n",
    "elif dat_source == 'hdf5':\n",
    "    h5_folder = '/e2ws_data/era5_73var'\n",
    "    data_source = hdf5.DataSource.from_path(\n",
    "        root=h5_folder, channel_names=model.in_channel_names\n",
    "    )\n",
    "    time = datetime.datetime(2017, 8, 24, 12)\n",
    "\n",
    "print(\"Running inference\")\n",
    "output_dir = \"outputs/04_diagnostic_precip\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "ds = run_basic_inference(\n",
    "    model_diagnostic,\n",
    "    n=36,\n",
    "    data_source=data_source,\n",
    "    time=time,\n",
    ")\n",
    "ds.to_netcdf(os.path.join(output_dir, \"precipitation_afno.nc\"))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "With inference complete we can do some post processing on our predictions.\n",
    "Hurricane Harvey was selected for inference due to its intense rain fall over\n",
    "the Gulf coast of Texas and Louisiana. Lets plot the total precipitation that\n",
    "occurred over Houston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "location = 'houston' # 'munich', 'san francisco'\n",
    "\n",
    "if location == 'san francisco':\n",
    "    loc_lat, loc_lon = 37.75, 237.5 # Lon is [0, 360]\n",
    "elif location == 'munich':\n",
    "    loc_lat, loc_lon = 48., 11.5\n",
    "elif location == 'houston':\n",
    "    loc_lat, loc_lon = 29.75, 264.75 # 84.75\n",
    "else:\n",
    "    raise ValueError(f'region {location} not implemented.')\n",
    "\n",
    "plt.close(\"all\")\n",
    "# Open dataset from saved NetCDFs\n",
    "ds = xr.open_dataarray(os.path.join(output_dir, \"precipitation_afno.nc\"))\n",
    "tp = ds.sel(channel=\"tp\", lat=loc_lat, lon=loc_lon)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(pd.to_datetime(tp.coords[\"time\"]), tp.values*1000)\n",
    "plt.title(f\"{location} (lat: {loc_lat}N lon: {loc_lon}E)\")\n",
    "plt.ylabel(\"Total Precipitation (mm)\")\n",
    "plt.savefig(f\"{output_dir}/tp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intense rain while Harvey was over Houston is clearly visible, lets have a look at the\n",
    "regional contour of the area to better understand the structure of this event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Open dataset from saved NetCDFs\n",
    "ds = xr.open_dataarray(os.path.join(output_dir, \"precipitation_afno.nc\"))\n",
    "nsteps = 4\n",
    "proj = ccrs.AlbersEqualArea(central_latitude=loc_lat, central_longitude=loc_lon)\n",
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    nsteps,\n",
    "    figsize=(20, 6.5),\n",
    "    subplot_kw={\"projection\": proj},\n",
    "    gridspec_kw={\"wspace\": 0.05, \"hspace\": 0.007},\n",
    ")\n",
    "if nsteps == 1:\n",
    "    ax = [ax]\n",
    "\n",
    "for step in range(nsteps):\n",
    "    i = step + 3\n",
    "    tp = ds[i, 0].sel(channel=\"tp\")\n",
    "\n",
    "    ax[step].add_feature(cartopy.feature.OCEAN, zorder=0)\n",
    "    ax[step].add_feature(cartopy.feature.LAND, zorder=0)\n",
    "    ax[step].add_feature(cartopy.feature.COASTLINE,lw=.5)\n",
    "    ax[step].add_feature(cartopy.feature.RIVERS,lw=.5)\n",
    "    ax[step].add_feature(cartopy.feature.BORDERS, linewidth=0.6, edgecolor='dimgray')\n",
    "\n",
    "    masked_data = np.ma.masked_where(tp.values < 0.001, tp.values)\n",
    "    img = ax[step].pcolormesh(\n",
    "        ds.lon,\n",
    "        ds.lat,\n",
    "        1000 * masked_data,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=\"plasma\",\n",
    "        vmin=0,\n",
    "        vmax=20,\n",
    "    )\n",
    "    ax[step].set_title(pd.to_datetime(ds.coords[\"time\"])[i])\n",
    "    ax[step].coastlines(color=\"k\")\n",
    "    ax[step].set_extent([loc_lon-10., loc_lon+10., loc_lat-7.5, loc_lat+7.5], ccrs.PlateCarree())\n",
    "    if step == nsteps-1:\n",
    "        plt.colorbar(img, ax=ax, shrink=.3, orientation=\"horizontal\",)\n",
    "\n",
    "\n",
    "ax[0].text(\n",
    "    -0.07,\n",
    "    0.55,\n",
    "    \"Total Precipitation (mm)\",\n",
    "    va=\"bottom\",\n",
    "    ha=\"center\",\n",
    "    rotation=\"vertical\",\n",
    "    rotation_mode=\"anchor\",\n",
    "    transform=ax[0].transAxes,\n",
    ")\n",
    "plt.tight_layout\n",
    "plt.savefig(f\"{output_dir}/diagnostic_bay_area_tp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><h4>Exercise</h4><p>\n",
    "\n",
    "In this notebook you have produced a global precipitation hindcast and\n",
    "analysed the rain over Houston. Feel free to explore other locations,\n",
    "San Francisco and Munich are already provided, you can also add further\n",
    "places which are close to your heart.\n",
    "\n",
    "Since some of you might have plans for a barbecue this week, let's check\n",
    "if the weather will be dry! To do so, change the data source to GFS and\n",
    "select a data in the near past, eg this morning. Can the plans go ahead,\n",
    "or should you do an indoor activity instead?\n",
    "\n",
    "</p></div>\n",
    "\n",
    "\n",
    "This completes the introductory notebook on running diagnostic models. Diagnostic\n",
    "models are significantly cheaper to train than generative models like CorrDiff. \n",
    "Such models have a range of other advantages though, which we will explore in \n",
    "the final exercise of this course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
